{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 2D VDS over 3D .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def npy_to_h5dataset(npyfilename, group, name):\n",
    "    \"\"\"Create a HDF5 dataset to access a .npy file\n",
    "\n",
    "    :param str npyfilename: Name of the .npy file.\n",
    "        WARNING: This filename is used as is to reference the .npy file\n",
    "        from the HDF5 file! Beware of absolute/relative path use.\n",
    "    :param h5py.Group group: HDF5 group where to create the dataset\n",
    "    :pram str name: Name of the HDF5 dataset\n",
    "    \"\"\"\n",
    "    with open(npyfilename, mode='rb') as f:\n",
    "        header = struct.unpack('6sBB', f.read(8))\n",
    "        assert header == (b'\\x93NUMPY', 1, 0)\n",
    "        length = struct.unpack('<H', f.read(2))[0]\n",
    "        f.seek(8)  # Seek back to end of magic header\n",
    "        shape, fortran_order, dtype = numpy.lib.format.read_array_header_1_0(f)        \n",
    "        assert not fortran_order\n",
    "\n",
    "    # Offset in bytes of the array\n",
    "    offset = length + 10\n",
    "    # Size in bytes of the array\n",
    "    size = numpy.prod(shape) * dtype.itemsize\n",
    "    \n",
    "    return group.create_dataset(\n",
    "        name,\n",
    "        shape=shape,\n",
    "        dtype=dtype,\n",
    "        external=[(os.path.abspath(npyfilename), offset, offset + size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HDF5 file a 3D external dataset stored in a .npy file and a 2D VDS mapping over it \n",
    "with h5py.File(\"/media/nvme/tvincent/testhdf5.h5\", mode=\"w\") as f:\n",
    "    # Add stack of images\n",
    "    images = npy_to_h5dataset('/media/nvme/tvincent/wasps_full.npy', f, 'images')\n",
    "\n",
    "    # Prepare VDS layout\n",
    "    assert images.ndim == 3\n",
    "    nbimgs = int(numpy.sqrt(images.shape[0]))\n",
    "    shape = nbimgs * images.shape[1], nbimgs * images.shape[2]\n",
    "\n",
    "    layout = h5py.VirtualLayout(shape=shape, dtype=images.dtype)\n",
    "    source = h5py.VirtualSource(images)\n",
    "    for row in range(nbimgs):\n",
    "        for col in range(nbimgs):\n",
    "            layout[row*images.shape[1]:(row+1)*images.shape[1],\n",
    "                   col*images.shape[2]:(col+1)*images.shape[2]] = source[row * nbimgs + col]\n",
    "\n",
    "    # Create VDS\n",
    "    f.create_virtual_dataset('level0', layout, fillvalue=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create binned levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(image):\n",
    "    assert image.ndim == 2\n",
    "    return 0.25 * (image[:-1:2, :-1:2] + image[:-1:2, 1::2] + image[1::2, :-1:2] + image[1::2, 1::2])\n",
    "\n",
    "def create_binned_level(group, name, previous, chunk=None):\n",
    "    \"\"\"Create a new HDF5 dataset with previous data binned 2x2\n",
    "\n",
    "    :param h5py.Group group:\n",
    "    :param str name:\n",
    "    :param Union[h5py.Dataset,numpy.ndarray] previous:\n",
    "    :param Union[None,List[int]] chunk:\n",
    "        If not None run process by chunks.\n",
    "        Chunk is defined in the output space.\n",
    "    \"\"\"\n",
    "    if chunk is None:\n",
    "        group[name] = binning(previous[()])\n",
    "        return group[name]\n",
    "\n",
    "    assert previous.ndim == 2\n",
    "    shape = previous.shape[0] // 2, previous.shape[1] // 2\n",
    "    dataset = group.create_dataset(\n",
    "        name, shape=shape, dtype=previous.dtype)\n",
    "\n",
    "    for row in range(shape[0] // chunk[0] + numpy.sign(shape[0] % chunk[0])):\n",
    "        for col in range(shape[1] // chunk[1] + numpy.sign(shape[1] % chunk[1])):\n",
    "            rbegin, rend = row * chunk[0], (row + 1) * chunk[0]\n",
    "            cbegin, cend = col * chunk[1], (col + 1) * chunk[1]\n",
    "            dataset[rbegin:rend, cbegin:cend] = binning(previous[rbegin*2:rend*2, cbegin*2:cend*2])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/media/nvme/tvincent/testhdf5.h5\", mode=\"a\") as f:\n",
    "    for index in range(1, 8):\n",
    "        print('Index', index)\n",
    "        create_binned_level(f, 'level%s' % index, previous=f['level%s' % (index-1)], chunk=(1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/media/nvme/tvincent/testhdf5.h5\", mode=\"r\") as f:\n",
    "    print(f['images'].shape, f['images'].dtype)\n",
    "    print(f['level0'].shape, f['level0'].dtype)\n",
    "    print(f['level7'].shape, f['level7'].dtype)\n",
    "    #plt.imshow(f['images'][1000].astype('float32'))\n",
    "    plt.imshow(f['level1'][:3445, :3445].astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate histogram\n",
    "\n",
    "For float16, generate the counts for each value (and clip to finite ones).\n",
    "This also works for uint16.\n",
    "\n",
    "For 32/64 bits, normal histogramming needed.\n",
    "\n",
    "Store histogram as `counts` and `values` along with the data: `level0_histo/counts` and `level0_histo/values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_float16_counts(data, chunk_shape=None):\n",
    "    \"\"\"Return counts of occurence of float16 values.\n",
    "\n",
    "    This allows fast histograming of float16.\n",
    "\n",
    "    :param Union[h5py.Dataset,numpy.ndarray] data:\n",
    "    :param Union[None,List[int]] chunk_shape:\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    # TODO support nD data and nD chunk_shape\n",
    "    assert data.dtype == numpy.float16\n",
    "    assert data.ndim >= 2\n",
    "\n",
    "    if chunk_shape is None:\n",
    "        chunk_shape = data.shape[:2]\n",
    "    assert len(chunk_shape) == 2\n",
    "\n",
    "    counts = numpy.zeros((2**16,), dtype=numpy.int64)\n",
    "\n",
    "    for row in range(0, data.shape[0], chunk_shape[0]):\n",
    "        for col in range(0, data.shape[1], chunk_shape[1]):\n",
    "            chunk = data[row:row+chunk_shape[0], col:col+chunk_shape[1]]\n",
    "            uint16_view = chunk.view(dtype=numpy.uint16)\n",
    "            uint16_view.shape = -1\n",
    "            counts = counts + numpy.bincount(uint16_view, minlength=counts.size)\n",
    "    return counts\n",
    "\n",
    "\n",
    "def get_finite_counts(*args, **kwargs):\n",
    "    \"\"\"Returns values and counts\n",
    "\n",
    "    :return: (values, counts)\n",
    "    :rtype: List[numpy.ndarray,numpy.ndarray]\n",
    "    \"\"\"\n",
    "    # TODO values and finite_mask can be cached\n",
    "    counts = get_float16_counts(*args, **kwargs)\n",
    "    values = numpy.arange(2**16, dtype=numpy.uint16).view(dtype=numpy.float16)\n",
    "\n",
    "    # Filter-out NaN and inf\n",
    "    finite_mask = numpy.isfinite(values)\n",
    "    values = values[finite_mask]\n",
    "    counts = counts[finite_mask]\n",
    "\n",
    "    # Sort returned values\n",
    "    indices = numpy.argsort(values)\n",
    "    return numpy.take(values, indices), numpy.take(counts, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Populate levels\n",
    "with h5py.File(\"/media/nvme/tvincent/testhdf5.h5\", mode=\"a\") as f:\n",
    "    for level in range(6, -1, -1):\n",
    "        level_name = 'level%d' % level\n",
    "        print(level_name)\n",
    "        group = f.create_group('%s_histo' % level_name)\n",
    "        values, counts = get_finite_counts(f[level_name], chunk_shape=(2048, 2048))\n",
    "        group['counts'] = counts\n",
    "        group['values'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Stats = namedtuple('Stats', ('min_', 'max_', 'mean', 'std', 'sum_', 'counts'))\n",
    "\n",
    "def get_stats_from_histo(values, counts):\n",
    "    \"\"\"Returns basic stats indicator computed from histogram\n",
    "\n",
    "    :param numpy.ndarray values:\n",
    "    :param numpy.ndarray counts:\n",
    "    :rtype: Stats\n",
    "    \"\"\"\n",
    "    subset = values[counts != 0]\n",
    "    min_, max_ = subset[0], subset[-1]\n",
    "    nbcounts = numpy.sum(counts, dtype=numpy.int64)\n",
    "    sum_ = numpy.sum(values * counts, dtype=numpy.float64)\n",
    "    mean = sum_ / nbcounts\n",
    "    std = numpy.sqrt(numpy.sum(counts * (values.astype(numpy.float64) - mean)**2) / nbcounts)\n",
    "    return Stats(\n",
    "        min_=subset[0],\n",
    "        max_=subset[-1],\n",
    "        sum_=sum_,\n",
    "        counts=nbcounts,\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with h5py.File(\"/media/nvme/tvincent/testhdf5.h5\", mode=\"r\") as f:\n",
    "    data = f['level4']\n",
    "    values, counts = get_finite_counts(data)\n",
    "    stats = get_stats_from_histo(values, counts)\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with h5py.File(\"/media/nvme/tvincent/testhdf5.h5\", mode=\"r\") as f:\n",
    "    data = f['level4'][()]\n",
    "\n",
    "Stats(\n",
    "        min_=numpy.min(data),\n",
    "        max_=numpy.max(data),\n",
    "        sum_=numpy.sum(data, dtype=numpy.float64),\n",
    "        counts=data.size,\n",
    "        mean=numpy.mean(data, dtype=numpy.float64),\n",
    "        std=numpy.std(data, dtype=numpy.float64),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update pyramid in live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(image):\n",
    "    assert image.ndim == 2\n",
    "    return 0.25 * (image[:-1:2, :-1:2] + image[:-1:2, 1::2] + image[1::2, :-1:2] + image[1::2, 1::2])\n",
    "\n",
    "def update_binned_levels(levels, previous, rbegin, rend, cbegin, cend):\n",
    "    \"\"\"Update sub-region of a pyramid of image\n",
    "\n",
    "    :param List[Union[h5py.Dataset,numpy.ndarray]] levels:\n",
    "        Iterable of pyramid levels to update from largest to smallest.\n",
    "    :param Union[h5py.Dataset,numpy.ndarray] previous:\n",
    "        Updated level which is the one before the first one of levels.\n",
    "    :param int rbegin:\n",
    "    :param int rend:\n",
    "    :param int cbegin:\n",
    "    :param int cend:\n",
    "    \"\"\"\n",
    "    for level in levels:\n",
    "        print(level)\n",
    "        # Start and end on even index, eventually enlarging the area \n",
    "        if rbegin % 2 != 0:\n",
    "            rbegin -= 1\n",
    "        if rend % 2 != 0:\n",
    "            rend += 1\n",
    "        if cbegin % 2 != 0:\n",
    "            cbegin -= 1\n",
    "        if cend % 2 != 0:\n",
    "            cend += 1\n",
    "\n",
    "        # TODO maybe fails on the edges\n",
    "        level[rbegin//2:rend//2, cbegin//2:cend//2] = binning(previous[rbegin:rend, cbegin:cend])\n",
    "        rbegin //= 2\n",
    "        rend //= 2\n",
    "        cbegin //= 2\n",
    "        cend //= 2\n",
    "        previous = level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Faster for smaller chunks, what else\n",
    "\n",
    "with h5py.File(\"/media/nvme/tvincent/testhdf5.h5\", mode=\"a\") as f:\n",
    "    for index in range(4):\n",
    "        rbegin, rend = 2048*index, 2048*(index+1)\n",
    "        cbegin, cend = 2048*index, 2048*(index+1)\n",
    "\n",
    "        update_binned_levels(\n",
    "            levels=[f['level%d' % i] for i in range(1, 8)],\n",
    "            previous=f['level0'],\n",
    "            rbegin=rbegin,\n",
    "            rend=rend,\n",
    "            cbegin=cbegin,\n",
    "            cend=cend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Raw image with NaN or 0\n",
    "# Fill level0 over time\n",
    "# Update other levels from level0\n",
    "# Notify?\n",
    "\n",
    "\n",
    "# Data/colormap update:\n",
    "# Store a histogram: edges, count for each level and use it for autoscale colormap.\n",
    "#   - Which number of bins?\n",
    "#   - float16 specific histo using count of each possible value\n",
    "#     (easy generation by using binary representation as index)\n",
    "# Send DATA event each time displayed level change? to update colormap/histo/profile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5glance import H5Glance\n",
    "H5Glance(\"/media/nvme/tvincent/testhdf5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data info\n",
    "with h5py.File(\"/media/nvme/tvincent/testhdf5.h5\", mode=\"r\") as f:\n",
    "    dataset = f['level0']\n",
    "    shape, dtype = dataset.shape, dataset.dtype\n",
    "\n",
    "# Create empty file\n",
    "with h5py.File(\"/media/nvme/tvincent/live_update.h5\", mode=\"w\") as f:\n",
    "    for index in range(0, 8):\n",
    "        print('Index', index, shape, dtype)\n",
    "        value = numpy.nan if dtype.kind == 'f' else 0\n",
    "        dataset = f.create_dataset(\n",
    "            'level%s' % index, shape=shape, dtype=dtype, fillvalue=value)\n",
    "        shape = shape[0] // 2, shape[1] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "chunk_shape = 1024, 1024\n",
    "\n",
    "# Get data shape\n",
    "with h5py.File(\"/media/nvme/tvincent/live_update.h5\", mode=\"r\") as f:\n",
    "    shape = f['level0'].shape\n",
    "\n",
    "for rbegin in range(0, shape[0], chunk_shape[0]):\n",
    "    for cbegin in range(1, shape[1], chunk_shape[1]):\n",
    "        rend = rbegin + chunk_shape[0]\n",
    "        cend = cbegin + chunk_shape[1]\n",
    "\n",
    "        # Read data\n",
    "        with h5py.File(\"/media/nvme/tvincent/testhdf5.h5\", mode=\"r\") as f:\n",
    "            data = f['level0'][rbegin:rend, cbegin:cend]\n",
    "\n",
    "        # Update pyramid file\n",
    "        with h5py.File(\"/media/nvme/tvincent/live_update.h5\", mode=\"a\") as f:\n",
    "            f['level0'][rbegin:rend, cbegin:cend] = data\n",
    "            update_binned_levels(\n",
    "                levels=[f['level%d' % i] for i in range(1, 8)],\n",
    "                previous=f['level0'],\n",
    "                rbegin=rbegin,\n",
    "                rend=rend,\n",
    "                cbegin=cbegin,\n",
    "                cend=cend)\n",
    "\n",
    "        # TODO update histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (venv)",
   "language": "python",
   "name": "python3-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
